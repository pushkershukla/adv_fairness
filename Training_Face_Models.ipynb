{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkershukla/adv_fairness/blob/main/Training_Face_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchattacks"
      ],
      "metadata": {
        "id": "T2oFb5RT2Kja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhvlrffK5nFD",
        "outputId": "dfec5fe4-dcd2-4e54-c9a8-fc526424db1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting logger\n",
            "  Downloading logger-1.4.tar.gz (1.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: logger\n",
            "  Building wheel for logger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for logger: filename=logger-1.4-py3-none-any.whl size=1780 sha256=2d4202854bc1e8eadc3c502983fb868cab921c38bdd2f595d3943c0924f80b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/19/7b/09fc73f7503166eaf7f31b4aa0095b7f78af2ec0898e1f8312\n",
            "Successfully built logger\n",
            "Installing collected packages: logger\n",
            "Successfully installed logger-1.4\n",
            "[01/May/2023 05:46:20] INFO - NumExpr defaulting to 2 threads.\n",
            "PyTorch version 2.0.0+cu118\n",
            "GPU-enabled installation? True\n"
          ]
        }
      ],
      "source": [
        "# all imports\n",
        "!pip install logger\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import PIL.Image as Image\n",
        "import logger\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from torch import optim \n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data as data\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import time\n",
        "import pandas\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# reproducibility\n",
        "seed = 42\n",
        "# SYSTEM\n",
        "np.random.seed(seed)\n",
        "#CUDA\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "# CUDNN\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# version\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"GPU-enabled installation? {}\".format(torch.cuda.is_available()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VStxUeG5xng"
      },
      "source": [
        "Check alloted GPU/CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YmHs4wB5wjD",
        "outputId": "7675024a-adf7-4a01-c371-309ee22f2c6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tue Mar 28 21:15:58 2023       ',\n",
              " '+-----------------------------------------------------------------------------+',\n",
              " '| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |',\n",
              " '|-------------------------------+----------------------+----------------------+',\n",
              " '| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |',\n",
              " '| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |',\n",
              " '|                               |                      |               MIG M. |',\n",
              " '|===============================+======================+======================|',\n",
              " '|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |',\n",
              " '| N/A   61C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |',\n",
              " '|                               |                      |                  N/A |',\n",
              " '+-------------------------------+----------------------+----------------------+',\n",
              " '                                                                               ',\n",
              " '+-----------------------------------------------------------------------------+',\n",
              " '| Processes:                                                                  |',\n",
              " '|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |',\n",
              " '|        ID   ID                                                   Usage      |',\n",
              " '|=============================================================================|',\n",
              " '|  No running processes found                                                 |',\n",
              " '+-----------------------------------------------------------------------------+']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!!nvidia-smi\n",
        "# preferred: Tesla V100-SXM2, CUDA v11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z3y-Rhm5_77",
        "outputId": "2e6c93b9-049c-4aee-89c8-d377dfe3c709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO0Lz3RN6ITg"
      },
      "source": [
        "# Download CelebA dataset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHkXfk276HyV",
        "outputId": "0d0f2904-96b4-4948-f779-264269eda28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZHA7NTr6XN_"
      },
      "outputs": [],
      "source": [
        "#!mkdir -p /content/gdrive/MyDrive/downloads && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip -P /content/gdrive/MyDrive/downloads "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFI30UVc7dKW"
      },
      "outputs": [],
      "source": [
        "dataset_dir='/content/gdrive/MyDrive/downloads/celeba/celeba.zip'\n",
        "\n",
        "with zipfile.ZipFile(dataset_dir,\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"faces/\")\n",
        "\n",
        "with zipfile.ZipFile(\"faces/celeba/img_align_celeba.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"faces/celeba\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orUGIsrp9_jF",
        "outputId": "a3175631-395b-4ed3-db4f-4f339157a522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "identity_CelebA.txt   list_attr_celeba.txt     list_landmarks_align_celeba.txt\n",
            "img_align_celeba      list_bbox_celeba.txt     list_landmarks_celeba.txt\n",
            "img_align_celeba.zip  list_eval_partition.txt\n"
          ]
        }
      ],
      "source": [
        "!ls faces/celeba\n",
        "#!ls "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "W9w-8JnMVN9d",
        "outputId": "6ab7f765-f482-4fad-e26f-e38c7f739a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "faces/celeba/img_align_celeba\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nceleba_train_dataset = torchvision.datasets.CelebA(\\n     image_path, split='train',\\n     target_type='attr', download=False\\n )\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# image_path=\"faces\"\n",
        "filepath=\"faces/celeba/list_attr_celeba.txt\"\n",
        "data_root='faces/celeba'\n",
        "dataset_folder = f'{data_root}/img_align_celeba'\n",
        "print(dataset_folder)\n",
        "'''\n",
        "\n",
        "celeba_train_dataset = torchvision.datasets.CelebA(\n",
        "     image_path, split='train',\n",
        "     target_type='attr', download=False\n",
        " )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(filepath, sep=\"\\s+\", skiprows=1, usecols=['Male'])\n",
        "df.loc[df['Male'] == -1, 'Male'] = 0\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iOj7aCji8gLv",
        "outputId": "15cf1be3-ab82-43a8-c161-83875b5fd373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Male\n",
              "000001.jpg     0\n",
              "000002.jpg     0\n",
              "000003.jpg     1\n",
              "000004.jpg     0\n",
              "000005.jpg     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24ccc828-b837-4436-be89-9e437ba6b88c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Male</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000001.jpg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000002.jpg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000003.jpg</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000004.jpg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000005.jpg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24ccc828-b837-4436-be89-9e437ba6b88c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24ccc828-b837-4436-be89-9e437ba6b88c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24ccc828-b837-4436-be89-9e437ba6b88c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_examples = int(df.shape[0]*0.8)\n",
        "df_train = df.iloc[:num_train_examples, :]\n",
        "df_test = df.iloc[num_train_examples:, :]\n",
        "print('Number of male and female images in training dataset:')\n",
        "np.bincount(df_train['Male'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIywR-fg9DT6",
        "outputId": "72fc7975-ff6a-46d6-d161-6560bc9d4b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of male and female images in training dataset:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([94093, 67986])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of male and female images in test dataset:')\n",
        "np.bincount(df_test['Male'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9R6u1dm9R3D",
        "outputId": "3ee58c99-fda2-4316-e959-ca0b95ee8da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of male and female images in test dataset:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24072, 16448])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CelebaDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
        "\n",
        "    def __init__(self, txt_path, img_dir, transform=None):\n",
        "    \n",
        "        df = pd.read_csv(filepath, sep=\"\\s+\", skiprows=1, usecols=['Male'])\n",
        "        self.img_dir = img_dir\n",
        "        self.txt_path = txt_path\n",
        "        self.img_names = df.index.values\n",
        "        self.y = df['Male'].values\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(os.path.join(self.img_dir,\n",
        "                                      self.img_names[index]))\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        label = self.y[index]\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.y.shape[0]"
      ],
      "metadata": {
        "id": "BqdxVszC-7ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "     transforms.RandomCrop([178, 178]),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.Resize([224, 224]),\n",
        "     transforms.ToTensor(),\n",
        " ])\n",
        "transform = transforms.Compose([\n",
        "     transforms.CenterCrop([178, 178]),\n",
        "     transforms.Resize([224, 224]),\n",
        "     transforms.ToTensor(),\n",
        " ])\n",
        "\n",
        "custom_transform = transforms.Compose([transforms.Grayscale(),                                       \n",
        "                                       #transforms.Lambda(lambda x: x/255.),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "dataset = CelebaDataset(txt_path=filepath,\n",
        "                              img_dir=dataset_folder,\n",
        "                              transform=transform_train)\n",
        "celeba_train_dataset,celeba_valid_dataset,celeba_test_dataset= torch.utils.data.random_split(dataset, [0.8,0.1,0.1])\n",
        "\n",
        "celeba_train_loader = DataLoader(dataset=celeba_train_dataset,\n",
        "                          batch_size=8,\n",
        "                          shuffle=True,\n",
        "                          num_workers=4) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWCCSXCc_Nfa",
        "outputId": "a939d892-f619-487d-ed84-a317f27b95f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "QEslbEbUVpqp",
        "outputId": "d5986543-5da9-4733-ec42-48ff8f43b148"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nceleba_valid_dataset = torchvision.datasets.CelebA(\\n     image_path, split='valid',\\n     target_type='attr', download=True\\n )\\n\\nceleba_test_dataset = torchvision.datasets.CelebA(\\n     image_path, split='test',\\n     target_type='attr', download=True\\n )\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "'''\n",
        "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
        "     image_path, split='valid',\n",
        "     target_type='attr', download=True\n",
        " )\n",
        "\n",
        "celeba_test_dataset = torchvision.datasets.CelebA(\n",
        "     image_path, split='test',\n",
        "     target_type='attr', download=True\n",
        " )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66anVcnf8cpQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bV0I1jEVzlr"
      },
      "outputs": [],
      "source": [
        "#print('Train set:', len(celeba_train_dataset))\n",
        "#print('Validation set:', len(celeba_valid_dataset))\n",
        "#print('Test set:', len(celeba_test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnzSLMIMUlDz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! ls faces/celeba/img_align_celeba"
      ],
      "metadata": {
        "id": "0I3COTtRIEbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Z5LB6CdcXG"
      },
      "source": [
        "# Smile Calssification Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "OuGUEZWsdXVH",
        "outputId": "fa2c5e99-096f-4b09-b39b-76feb1833f5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom torch.utils.data import DataLoader\\nget_smile = lambda attr: attr[31]\\nv=get_smile\\nprint(type(v))\\ntransform_train = transforms.Compose([\\n     transforms.RandomCrop([178, 178]),\\n     transforms.RandomHorizontalFlip(),\\n     transforms.Resize([224, 224]),\\n     transforms.ToTensor(),\\n ])\\ntransform = transforms.Compose([\\n     transforms.CenterCrop([178, 178]),\\n     transforms.Resize([224, 224]),\\n     transforms.ToTensor(),\\n ])\\n\\nceleba_train_dataset_smile = torchvision.datasets.CelebA(\\n     image_path, split='train',\\n     target_type='attr', download=False,\\n     transform=transform_train#, target_transform=get_smile\\n )\\ntorch.manual_seed(1)\\ndata_loader_smile = DataLoader(celeba_train_dataset_smile, batch_size=16)\\nceleba_valid_dataset_smile = torchvision.datasets.CelebA(\\n     image_path, split='valid',\\n     target_type='attr', download=False,\\n     transform=transform#, target_transform=get_smile\\n )\\nceleba_test_dataset_smile = torchvision.datasets.CelebA(\\n     image_path, split='test',\\n     target_type='attr', download=False,\\n     transform=transform#, target_transform=get_smile\\n )\\n#Training our model on 16000 dataponints initially  and 1000 validation poitns\\n#Change this later\\nfrom torch.utils.data import Subset\\nceleba_train_dataset = Subset(celeba_train_dataset_smile,\\n                               torch.arange(50000))\\nceleba_valid_dataset = Subset(celeba_valid_dataset_smile,\\n                               torch.arange(5000))\\nprint('Train set:', len(celeba_train_dataset))\\n\\nprint('Validation set:', len(celeba_valid_dataset))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "'''\n",
        "from torch.utils.data import DataLoader\n",
        "get_smile = lambda attr: attr[31]\n",
        "v=get_smile\n",
        "print(type(v))\n",
        "transform_train = transforms.Compose([\n",
        "     transforms.RandomCrop([178, 178]),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.Resize([224, 224]),\n",
        "     transforms.ToTensor(),\n",
        " ])\n",
        "transform = transforms.Compose([\n",
        "     transforms.CenterCrop([178, 178]),\n",
        "     transforms.Resize([224, 224]),\n",
        "     transforms.ToTensor(),\n",
        " ])\n",
        "\n",
        "celeba_train_dataset_smile = torchvision.datasets.CelebA(\n",
        "     image_path, split='train',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform_train#, target_transform=get_smile\n",
        " )\n",
        "torch.manual_seed(1)\n",
        "data_loader_smile = DataLoader(celeba_train_dataset_smile, batch_size=16)\n",
        "celeba_valid_dataset_smile = torchvision.datasets.CelebA(\n",
        "     image_path, split='valid',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform#, target_transform=get_smile\n",
        " )\n",
        "celeba_test_dataset_smile = torchvision.datasets.CelebA(\n",
        "     image_path, split='test',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform#, target_transform=get_smile\n",
        " )\n",
        "#Training our model on 16000 dataponints initially  and 1000 validation poitns\n",
        "#Change this later\n",
        "from torch.utils.data import Subset\n",
        "celeba_train_dataset = Subset(celeba_train_dataset_smile,\n",
        "                               torch.arange(50000))\n",
        "celeba_valid_dataset = Subset(celeba_valid_dataset_smile,\n",
        "                               torch.arange(5000))\n",
        "print('Train set:', len(celeba_train_dataset))\n",
        "\n",
        "print('Validation set:', len(celeba_valid_dataset))\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0unqDEhe1-Ap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "969ecdb3-a92e-46e0-9da7-e6a745955476"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom torch.utils.data import DataLoader\\nget_gender = lambda attr: attr[21]\\n#a =get_gender\\n\\nceleba_train_dataset_gender = torchvision.datasets.CelebA(\\n     image_path, split='train',\\n     target_type='attr', download=False,\\n     transform=transform_train#, target_transform=get_gender\\n )\\ntorch.manual_seed(1)\\ndata_loader_smile = DataLoader(celeba_train_dataset_gender, batch_size=16)\\nceleba_valid_dataset_gender = torchvision.datasets.CelebA(\\n     image_path, split='valid',\\n     target_type='attr', download=False,\\n     transform=transform#, target_transform=get_gender\\n )\\nceleba_test_dataset_gender = torchvision.datasets.CelebA(\\n     image_path, split='test',\\n     target_type='attr', download=False,\\n     transform=transform#, target_transform=get_gender\\n )\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "'''\n",
        "from torch.utils.data import DataLoader\n",
        "get_gender = lambda attr: attr[21]\n",
        "#a =get_gender\n",
        "\n",
        "celeba_train_dataset_gender = torchvision.datasets.CelebA(\n",
        "     image_path, split='train',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform_train#, target_transform=get_gender\n",
        " )\n",
        "torch.manual_seed(1)\n",
        "data_loader_smile = DataLoader(celeba_train_dataset_gender, batch_size=16)\n",
        "celeba_valid_dataset_gender = torchvision.datasets.CelebA(\n",
        "     image_path, split='valid',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform#, target_transform=get_gender\n",
        " )\n",
        "celeba_test_dataset_gender = torchvision.datasets.CelebA(\n",
        "     image_path, split='test',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform#, target_transform=get_gender\n",
        " )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1e9qphD1qhJ"
      },
      "outputs": [],
      "source": [
        "def comp_model(model1,model2):\n",
        "  for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
        "      if p1.data.ne(p2.data).sum() > 0:\n",
        "          return False\n",
        "  return True\n",
        "#print(\"Comparing models\",comp_model(model_gender,model_smile))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(model_1, model_2):\n",
        "    models_differ = 0\n",
        "    for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()):\n",
        "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
        "            pass\n",
        "        else:\n",
        "            models_differ += 1\n",
        "            if (key_item_1[0] == key_item_2[0]):\n",
        "                print('Mismtach found at', key_item_1[0])\n",
        "            else:\n",
        "                raise Exception\n",
        "    if models_differ == 0:\n",
        "        print('Models match perfectly! :)')\n"
      ],
      "metadata": {
        "id": "DHZ_VBke0a7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_state_dicts(model_state_dict_1, model_state_dict_2):\n",
        "    if len(model_state_dict_1) != len(model_state_dict_2):\n",
        "        logger.info(\n",
        "            f\"Length mismatch: {len(model_state_dict_1)}, {len(model_state_dict_2)}\"\n",
        "        )\n",
        "        return False\n",
        "\n",
        "    # Replicate modules have \"module\" attached to their keys, so strip these off when comparing to local model.\n",
        "    if next(iter(model_state_dict_1.keys())).startswith(\"module\"):\n",
        "        model_state_dict_1 = {\n",
        "            k[len(\"module\") + 1 :]: v for k, v in model_state_dict_1.items()\n",
        "        }\n",
        "\n",
        "    if next(iter(model_state_dict_2.keys())).startswith(\"module\"):\n",
        "        model_state_dict_2 = {\n",
        "            k[len(\"module\") + 1 :]: v for k, v in model_state_dict_2.items()\n",
        "        }\n",
        "\n",
        "    for ((k_1, v_1), (k_2, v_2)) in zip(\n",
        "        model_state_dict_1.items(), model_state_dict_2.items()\n",
        "    ):\n",
        "        if k_1 != k_2:\n",
        "            logger.info(f\"Key mismatch: {k_1} vs {k_2}\")\n",
        "            return False\n",
        "        # convert both to the same CUDA device\n",
        "        if str(v_1.device) != \"cuda:0\":\n",
        "            v_1 = v_1.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if str(v_2.device) != \"cuda:0\":\n",
        "            v_2 = v_2.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        if not torch.allclose(v_1, v_2):\n",
        "            logger.info(f\"Tensor mismatch: {v_1} vs {v_2}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "ijQdUaUbz3WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Jzzv_02lB4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5737749d-a80d-4930-c9ca-da0c318b0f5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbatch_size = 32\\ntorch.manual_seed(1)\\ngender_train_dl = DataLoader(celeba_train_dataset_gender,\\n                       batch_size, shuffle=True)\\ngender_valid_dl = DataLoader(celeba_valid_dataset_gender,\\n                       batch_size, shuffle=False)\\ngender_test_dl = DataLoader(celeba_test_dataset_gender,\\n                     batch_size, shuffle=False)\\n                     '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "'''\n",
        "batch_size = 32\n",
        "torch.manual_seed(1)\n",
        "gender_train_dl = DataLoader(celeba_train_dataset_gender,\n",
        "                       batch_size, shuffle=True)\n",
        "gender_valid_dl = DataLoader(celeba_valid_dataset_gender,\n",
        "                       batch_size, shuffle=False)\n",
        "gender_test_dl = DataLoader(celeba_test_dataset_gender,\n",
        "                     batch_size, shuffle=False)\n",
        "                     '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "get_smile = lambda attr: attr[31]\n",
        "smile_attr_idx=31\n",
        "gender_attr_idx=20\n",
        "'''\n",
        "celeba_train_dataset = torchvision.datasets.CelebA(\n",
        "     image_path, split='train',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform_train\n",
        " )\n",
        "\n",
        "\n",
        "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
        "     image_path, split='valid',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform\n",
        " )\n",
        "celeba_test_dataset = torchvision.datasets.CelebA(\n",
        "     image_path, split='test',\n",
        "     target_type='attr', download=False,\n",
        "     transform=transform\n",
        " )\n",
        "#Training our model on 16000 dataponints initially  and 1000 validation poitns\n",
        "#Change this later\n",
        "from torch.utils.data import Subset\n",
        "celeba_train_dataset = Subset(celeba_train_dataset,\n",
        "                               torch.arange(50000))\n",
        "celeba_valid_dataset = Subset(celeba_valid_dataset,\n",
        "                               torch.arange(5000))\n",
        "print('Train set:', len(celeba_train_dataset))\n",
        "\n",
        "print('Validation set:', len(celeba_valid_dataset))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "dM88UUQFEZQj",
        "outputId": "50fab951-4a75-41df-d64d-3432efc1d035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nceleba_train_dataset = torchvision.datasets.CelebA(\\n     image_path, split='train',\\n     target_type='attr', download=False,\\n     transform=transform_train\\n )\\n\\n\\nceleba_valid_dataset = torchvision.datasets.CelebA(\\n     image_path, split='valid',\\n     target_type='attr', download=False,\\n     transform=transform\\n )\\nceleba_test_dataset = torchvision.datasets.CelebA(\\n     image_path, split='test',\\n     target_type='attr', download=False,\\n     transform=transform\\n )\\n#Training our model on 16000 dataponints initially  and 1000 validation poitns\\n#Change this later\\nfrom torch.utils.data import Subset\\nceleba_train_dataset = Subset(celeba_train_dataset,\\n                               torch.arange(50000))\\nceleba_valid_dataset = Subset(celeba_valid_dataset,\\n                               torch.arange(5000))\\nprint('Train set:', len(celeba_train_dataset))\\n\\nprint('Validation set:', len(celeba_valid_dataset))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "torch.manual_seed(1)\n",
        "train_dl = DataLoader(celeba_train_dataset,\n",
        "                       batch_size, shuffle=True)\n",
        "valid_dl = DataLoader(celeba_valid_dataset,\n",
        "                       batch_size, shuffle=False)\n",
        "test_dl = DataLoader(celeba_test_dataset,\n",
        "                     batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "9iOVUpmaGKKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "658_qHPE2JQ9"
      },
      "outputs": [],
      "source": [
        "model_name = \"resnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "feature_extract=True\n",
        "num_epochs=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR4PJ3Ve2-rW"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d15ka50VVhGp",
        "outputId": "8029f368-d4db-442c-8498-c45ffc739b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained='vggface2')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        model_ft.fc=nn.Sequential(\n",
        "                model_ft.fc,\n",
        "                nn.Softmax(),\n",
        "                )\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained='vggface2')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained='vggface2')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained='vggface2')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained='vggface2')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained='vggface2')\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "##model_smile, input_size = initialize_model(model_name, num_classes, feature_extract=True, use_pretrained=True)\n",
        "model_gender,_= initialize_model(model_name, num_classes, feature_extract=True, use_pretrained=True)\n",
        "#print(type(model_smile))\n",
        "#model_smile = model_smile.to(device)\n",
        "#model_gender =model_gender.to(device)\n",
        "# Print the model we just instantiated\n",
        "#compare_models(model_gender,model_smile)\n",
        "#print(validate_state_dicts(model_gender.state_dict(),model_smile.state_dict()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8W5qaJa3kmF"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e01ksBMS3l9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f53b017-66d2-48a5-be27-a2f16c6ca890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model_gender)\n",
        "model_gender=model_gender.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oE1_KG3VhGr"
      },
      "outputs": [],
      "source": [
        "# Send the model to GPU\n",
        "\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are \n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "'''\n",
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "'''\n",
        "# Observe that all parameters are being optimized\n",
        "def return_optimizer(model):\n",
        "    return  optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37CUt06D34mc"
      },
      "outputs": [],
      "source": [
        "dataloaders_dict ={'train':train_dl,'val':valid_dl}\n",
        "#gender_dataloader_dict={'train':gender_train_dl,'val':gender_valid_dl}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T5FidCGVhGk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, type,num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels= labels.to(device)\n",
        "                #print( \"Smile \",labels[:,smile_attr_idx])\n",
        "                #print(\"Gender\",labels[:,gender_attr_idx])\n",
        "                '''\n",
        "                if (type=='smile'):\n",
        "\n",
        "                 labels = labels[:,smile_attr_idx].squeeze().to(device)\n",
        "                if (type=='gender'):\n",
        "\n",
        "                 labels = labels[:,gender_attr_idx].squeeze().to(device)\n",
        "                '''\n",
        "                print(\"Label\",(labels))\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        print(outputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        #print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5zxi0CI4GQn"
      },
      "outputs": [],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7BjH3YkQVhGs",
        "outputId": "65cd8d86-e931-4e7d-c1fa-c77b523a6a45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nsmile_optimizer=return_optimizer(model_smile)\\n# Train and evaluate\\nmodel_smile, hist = train_model(model_smile, dataloaders_dict, criterion, smile_optimizer,'smile', num_epochs=num_epochs, is_inception=False)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "smile_optimizer=return_optimizer(model_smile)\n",
        "# Train and evaluate\n",
        "model_smile, hist = train_model(model_smile, dataloaders_dict, criterion, smile_optimizer,'smile', num_epochs=num_epochs, is_inception=False)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gender_optimizer=return_optimizer(model_gender)\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_gender, dataloaders_dict, criterion, gender_optimizer,'gender', num_epochs=num_epochs, is_inception=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "qi8_yZRK1xXA",
        "outputId": "328598e9-735a-4d59-99fe-a1496c405310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "Label tensor([-1,  1, -1,  1,  1, -1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5646, 0.4354],\n",
            "        [0.5256, 0.4744],\n",
            "        [0.6759, 0.3241],\n",
            "        [0.7457, 0.2543],\n",
            "        [0.6889, 0.3111],\n",
            "        [0.6088, 0.3912],\n",
            "        [0.6450, 0.3550],\n",
            "        [0.6208, 0.3792],\n",
            "        [0.7197, 0.2803],\n",
            "        [0.6849, 0.3151],\n",
            "        [0.8442, 0.1558],\n",
            "        [0.5369, 0.4631],\n",
            "        [0.6109, 0.3891],\n",
            "        [0.5874, 0.4126],\n",
            "        [0.6406, 0.3594],\n",
            "        [0.6823, 0.3177]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-62cb3aef9300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgender_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-819f7599e0ca>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, type, num_epochs, is_inception)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models(model_smile,model_gender)"
      ],
      "metadata": {
        "id": "VOK12eR32Aiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHdwvJ6v6WTQ"
      },
      "outputs": [],
      "source": [
        "smile_model_name=\"smile_model3.pt\"\n",
        "save_path=\"/content/gdrive/MyDrive/FairFaceRecExptResults/\"+smile_model_name\n",
        "torch.save(model_smile.state_dict(),save_path )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gender_model=copy.deepcopy(model_ft)\n",
        "\n",
        "\n",
        "\n",
        "gender_model_name=\"gender_model3.pt\"\n",
        "save_path=\"/content/gdrive/MyDrive/FairFaceRecExptResults/\"+gender_model_name\n",
        "torch.save(model_gender.state_dict(),save_path )\n",
        "\n"
      ],
      "metadata": {
        "id": "x-UEUn1-9nco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(model_1, model_2):\n",
        "    models_differ = 0\n",
        "    for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()):\n",
        "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
        "            pass\n",
        "        else:\n",
        "            models_differ += 1\n",
        "            if (key_item_1[0] == key_item_2[0]):\n",
        "                print('Mismtach found at', key_item_1[0])\n",
        "            else:\n",
        "                raise Exception\n",
        "    if models_differ == 0:\n",
        "        print('Models match perfectly! :)')\n",
        "compare_models(gender_model,model_ft)"
      ],
      "metadata": {
        "id": "bI01t6VXj3u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6nlGMw27KY5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#model = model_ft\n",
        "#model = model.to(device)\n",
        "#model_weights = torch.load(save_path,map_location=device)\n",
        "#model.load_state_dict(model_weights)\n",
        "#model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtbkBsN1Vq9b"
      },
      "outputs": [],
      "source": [
        "# deep Copying a model \n",
        "model_gender=copy.deepcopy(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do this after adding the new layer\n",
        "#model_ft, hist = train_model(model_gender, dataloaders_dict, criterion, optimizer_ft,'gender', num_epochs=num_epochs, is_inception=(model_name==\"densenet\"))"
      ],
      "metadata": {
        "id": "zNWQyG8AKU32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RmQvAlkMilB"
      },
      "outputs": [],
      "source": [
        "labels=[\"smiling\",\"not smiling\"]\n",
        "def predict(input_img,model=model):\n",
        "    \n",
        "    input=transform(input_img)\n",
        "    input = input.unsqueeze(0)\n",
        "    input = input.to(device)\n",
        "    model= model.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      prediction = torch.nn.functional.softmax(model(input), dim=1)\n",
        "      output=model(input)\n",
        "      print(\"output\",prediction,model(input))\n",
        "      #print(\" Predcition only \",)\n",
        "      predcition=prediction.cpu().numpy()\n",
        "      print(prediction)\n",
        "      confidences= {labels[i]:float(prediction[i]) for i in range (0,len(labels))}\n",
        "\n",
        "    #idx2class={v: k for v, k in enumerate(dataframe.columns[1:].values)}\n",
        "    #confidences = {idx2class[i]: float(prediction[i]) for i in idx2class.values()}  \n",
        "    return  confidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb8KuPOIYOIU"
      },
      "outputs": [],
      "source": [
        "class new_model (nn.Module):\n",
        "  def __init__(self, output_layer,pretrained_model,num_classes=2,feature_extract=True):\n",
        "    super().__init__()\n",
        "    self.pretrained_model=pretrained_model\n",
        "    self.output_layer=output_layer\n",
        "    self.layers=list(self.pretrained_model._modules.keys())\n",
        "    self.layer_count=0\n",
        "    self.feature_extract=feature_extract\n",
        "    self.num_classes=num_classes\n",
        "    set_parameter_requires_grad(pretrained_model,self.feature_extract)\n",
        "\n",
        "    for l in self.layers:\n",
        "          print(self.layers)\n",
        "          if l != self.output_layer:\n",
        "                self.layer_count += 1\n",
        "          else:\n",
        "                break\n",
        "    for i in range(1,len(self.layers)-self.layer_count):\n",
        "          self.dummy_var = self.pretrained_model._modules.pop(self.layers[-i])\n",
        "    #last_layer_shape=self.layers[-1].shape \n",
        "    #print(last_layer_shape)\n",
        "    #num_features=1\n",
        "    #for i in range(1,len(last_layer_shape)):\n",
        "      #num_features=num_features*last_layer_shape[i]\n",
        "    #self.num_features=self.layers[-1].shape[1]*self.layers\n",
        "    self.classification_layer=nn.Sequential(torch.nn.LazyLinear(num_classes))\n",
        "    self.net = nn.Sequential(self.pretrained_model._modules)\n",
        "    self.pretrained = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.net(x)\n",
        "    #print(x.shape)\n",
        "    batch_size=x.shape[0]\n",
        "\n",
        "    x=torch.flatten(x,start_dim=1)\n",
        "    #print(x.shape)\n",
        "    num_features=x.shape[1]\n",
        "    #classification_layer=nn.Linear(num_features,self.num_classes).to(device)\n",
        "    #print(classification_layer)\n",
        "    #x=classification_layer(nn.Sex)\n",
        "    #x=torch.nn.functional.softmax(x)\n",
        "    #print(x.shape)\n",
        "    x=self.classification_layer(x)\n",
        "    #print(x.shape)\n",
        "    return (x)\n",
        "# Test case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnEW4Ijj6eEN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTo5n9urqZCe"
      },
      "outputs": [],
      "source": [
        "gender_model=new_model('avgpool',model_gender)\n",
        "gender_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLSGQk0Q5h0D"
      },
      "outputs": [],
      "source": [
        "gender_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gender_model2,_=initialize_model(model_name, num_classes, feature_extract=True, use_pretrained=True)"
      ],
      "metadata": {
        "id": "OVofRVFTO38W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKRb8gnC4smR"
      },
      "outputs": [],
      "source": [
        "params_to_update = gender_model2.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in gender_model2.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in gender_model2.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlQG6i0HpwAI"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "gender_model2.to(device)\n",
        "gender_model2, hist = train_model(gender_model2, gender_dataloader_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"densenet\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcSEXQMn_G6b"
      },
      "outputs": [],
      "source": [
        "gender_model_name=\"gender_model_real.pt\"\n",
        "save_path=\"/content/gdrive/MyDrive/FairFaceRecExptResults/\"+gender_model_name\n",
        "torch.save(gender_model.state_dict(),save_path )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERWQKSAQ-CRm"
      },
      "outputs": [],
      "source": [
        "! pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etneCfd19x17"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo=gr.Interface(fn=predict, \n",
        "             inputs=gr.Image(type=\"pil\"),\n",
        "             outputs=gr.Label())\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmsWE7ZDL9nRrmlGEOvSVw",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}